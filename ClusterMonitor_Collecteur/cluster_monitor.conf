#!/bin/bash -x
 
# Plugins cluster-monitor slurm ( <= slurm 17.11.2 ) 	
# Information : le cluster doit être d'abord insérer en base côté server
# Paquets nécéssaire : 	ipmitool / clustershell	/ mysql-client
# Autheur : Montagne Vincent / Gilbert Nicolas
# Version : 1.5.5
# Date : 12 juin 2018
# Path install modifiable ( cluster_monitor_daemon/PATHINST=... et cluster_monitor.sh/WORK_DIR=...) 
# Daemon : lien dans /etc/init.d/cluster_monitor /srv/cluster_monitor/cluster_monitor_daemon

# --------------------- Definition des variables d'administration --------------------- #

# Fichier des fichiers et répertoire de travail.
LOG_FILE="/var/log/clustermonitor.log"

TEMP_DIR="${WORK_DIR}/tmp"
if [[ ! -e ${TEMP_DIR} ]] ;then mkdir ${TEMP_DIR} ;fi

TEMP_FILE_TOPO="${TEMP_DIR}/topology.txt"
TEMP_FILE_NODE="${TEMP_DIR}/node.txt"
TEMP_FILE_NODE_HARD="${TEMP_DIR}/node_hard.txt"
TEMP_FILE_BDD="${TEMP_DIR}/bdd.txt"
TEMP_FILE_BDD_TOPOCNS="${TEMP_DIR}/bddtopocns.txt"
TEMP_FILE_CMD_QUOTA="${TEMP_DIR}/cmd_quota.txt"


# Information de connexion à la base de données server.
USERBDD=""
MDPBDD=""
IPBDD=""
PORTBDD=""
BDD="cluster_monitor"
IS_SSL="on"
SSLCA="${WORK_DIR}/ssl/ca-cert.pem"
SSLCERT="${WORK_DIR}/ssl/client-cert.pem"
SSLKEY="${WORK_DIR}/ssl/client-key.pem"
if [ "${IS_SSL}" = "on" ];then
	SSLCONF="--ssl-ca="${SSLCA}" --ssl-cert="${SSLCERT}" --ssl-key="${SSLKEY}""
fi


# Lock file
LOCKFILE="${WORK_DIR}/cluster-monitor.lock"
LOCKFILETOPOCNS="${WORK_DIR}/cluster-monitor-topocns.lock"

# Console log
OK="[\033[0;32mok\033[0m]"
FAIL="[\033[0;31mFailed\033[0m]"
FC="\033[0m"

# Mise en place des remontÃs des Metrics des jobs ( yes or no )
JOBSMETRICS="no"

# --------------------- Definition des variables de travail --------------------- #
INTERCONNECT=""

# Nom du cluster ( /!\ le nom doit être identique à celui en base de donnée côté server, respecter la case ).
CLUSTER=""

# Nom du gestionnaire batch ( ex: "slurm" ).
BATCHSCHEDULER="slurm"

# Fichier conf slurm ( en cas de multicluster ).
export SLURM_CONF="/etc/slurm-llnl/slurm.conf"
# Nom de la partition slurm incluant tous les noeuds de calcul.
PARTITION_CLUSTER="all"
# Report wckeys yes/no
REPORTWCKEYS="yes"

# Systeme de fichiers a surveiller ( ex: "scratch|mount_point home|mount_point" ).
FILESYSTEM=""

# Liste Frontaux du cluster ( ex: "frontal1 frontal2" ).
FRONTAUX=""

# Liste des noeuds de services a surveiller.
NOEUDS_SERVICE=""

# Ldap ( fonction collectUsers à commenté si non utilisé )
# Nom du serveur Ldap ( slapcat via ssh ) pour recuperer les informations des utilisateurs 
SRVLDAP=""
# Liste des groupes des utilisateurs du cluster ( ex: "groupe1 groupe2" ).
GROUPE=""
OUGROUPE=""

# wrapping de commande
MYSQL="mysql -h ${IPBDD} -P ${PORTBDD} -u${USERBDD} -p${MDPBDD} -D ${BDD} -N ${SSLCONF} -e" 
MYSQL_FILE="mysql -h ${IPBDD} -P ${PORTBDD} -u${USERBDD} -p${MDPBDD} -D ${BDD} -N ${SSLCONF}"
DF="timeout -s 9 5s df"
CMDVDGB="sinfo -V > /dev/null 2>&1" 		# Commande de vérification de la disponibilitée du gestionnaire de batch
CMDCONFGB="scontrol show config"		# Commande de récupération configuration du gestionnaire de batch
CMDVGB="sinfo --version  | awk '{print$2}'"	# Commande de récupération version du gestionnaire de batch

# Quota xfs ( COLLECTQUOTAXFS="yes/no" )
# Noeud permettant de recuperer l'ensemble des quotas xfs + volume et nom fs (vghome1|name1|type1 vghome2|name2|type2)
# Type = p project, u user, g group
COLLECTQUOTAXFS="no"
SRVNFS=""
VOLFSXFS=""

# Quota gpfs ( COLLECTQUOTAGPFS="yes/no" )
# Noeud permettant de recuperer l'ensemble des quotas gpfs + volume et nom fs (vghome1|name1|type vghome2|name2|type)
# type => user/group
COLLECTQUOTAGPFS="yes"
SRVGPFS=""
VOLFSGPFS=""

# Quota lustre ( COLLECTQUOTALUSTRE="yes/no" )
# Noeud permettant de recuperer l'ensemble des quotas lustre + volume et nom fs (vghome1|name1 vghome2|name2)
COLLECTQUOTALUSTRE="no"
SRVLUSTRE=""
VOLLUSTRE=""

